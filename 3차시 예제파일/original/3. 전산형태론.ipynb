{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "짧은 단어 삭제:  was wondering anyone out there could enlighten this car.\n",
      "\n",
      "단어 토큰화 1: ['Time', 'is', 'an', 'illusion', 'Lunchtime', 'double', 'so']\n",
      "\n",
      "단어 토큰화 2: ['Starting', 'a', 'home', 'based', 'restaurant', 'may', 'be', 'an', 'ideal', 'it', 'doesn', 't', 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'I was wondering if anyone out there could enlighten me on this car.'\n",
    "\n",
    "# 문장을 처리하는 방법 1\n",
    "# 2자 이내의 짧은 단어를 삭제합니다.\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "data = shortword.sub('', sentence)\n",
    "\n",
    "print('짧은 단어 삭제:', data)\n",
    "\n",
    "# 문장을 처리하는 방법 2\n",
    "# 단어를 기반으로 토큰화를 해봅시다.\n",
    "tokenizer = RegexpTokenizer('[\\w]+')\n",
    "sentence = 'Time is an illusion. Lunchtime double so!'\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "\n",
    "print('\\n단어 토큰화 1:', tokens)\n",
    "\n",
    "sentence = \"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\"\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print('\\n단어 토큰화 2:', tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "트리뱅크 워드토크나이저 : ['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n",
      "품사 태깅 : [('Starting', 'VBG'), ('a', 'DT'), ('home-based', 'JJ'), ('restaurant', 'NN'), ('may', 'MD'), ('be', 'VB'), ('an', 'DT'), ('ideal.', 'NN'), ('it', 'PRP'), ('does', 'VBZ'), (\"n't\", 'RB'), ('have', 'VB'), ('a', 'DT'), ('food', 'NN'), ('chain', 'NN'), ('or', 'CC'), ('restaurant', 'NN'), ('of', 'IN'), ('their', 'PRP$'), ('own', 'JJ'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\nsun5\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['restaurant', 'ideal.', 'food', 'chain', 'restaurant']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 품사 분석을 위한 태그를 다운받습니다.\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "postag = pos_tag(tokens)\n",
    "\n",
    "print('트리뱅크 워드토크나이저 :', tokens)\n",
    "print('품사 태깅 :', postag)\n",
    "\n",
    "noun_list = [x for (x, y) in postag if y == 'NN']\n",
    "noun_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어간 추출 전 : ['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n",
      "\n",
      "어간 추출 후 : ['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n",
      "\n",
      "=====================\n",
      "\n",
      "어간 추출 전 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
      "\n",
      "포터 스테머의 어간 추출 후: ['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n",
      "\n",
      "랭커스터 스테머의 어간 추출 후: ['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "sentence = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things with the single exception of the red crosses and the written notes.\"\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "\n",
    "# tokens를 순회하며 어간 추출 \n",
    "data = [stemmer.stem(data) for data in tokens]\n",
    "\n",
    "print('어간 추출 전 :', tokens)\n",
    "print('\\n어간 추출 후 :', data)\n",
    "print('\\n=====================\\n')\n",
    "\n",
    "# 다른 어간추출기와 비교\n",
    "ps = PorterStemmer()\n",
    "ls = LancasterStemmer()\n",
    "\n",
    "wordlist = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "\n",
    "porterdata = [ps.stem(data) for data in wordlist]\n",
    "lancasterdata = [ls.stem(data) for data in wordlist]\n",
    "\n",
    "print('어간 추출 전 :', wordlist)\n",
    "print('\\n포터 스테머의 어간 추출 후:',porterdata)\n",
    "print('\\n랭커스터 스테머의 어간 추출 후:',lancasterdata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20efb7bbc3e880fa5daaecac3f0f42ac0fcde1e987afd03e25384b1713bb4579"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
